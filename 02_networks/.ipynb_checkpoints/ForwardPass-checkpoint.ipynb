{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a52c57a2",
   "metadata": {},
   "source": [
    "# Implementing the Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e99160",
   "metadata": {},
   "source": [
    "Let's use the ```__call__()``` module within the layers of the network to implement the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "67972a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.set_printoptions(linewidth=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56acb69",
   "metadata": {},
   "source": [
    "Let's use the ```nn.Module``` class and initialise the parent class using the ```super()``` constructor. We will define a convolutional network with:\n",
    "- Convolutional layer with 6 output channels\n",
    "- Convolutional layer with 12 output channels\n",
    "- Dense layer with 120 output nodes\n",
    "- Dense layer with 60 output nodes\n",
    "- Dense layer with 10 output nodes\n",
    "\n",
    "Let us also equip each layer with the ReLU activation function, that is available from ```torch.nn.functional```, and each convolutional layer with a maxpooling operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f0c3b135",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ConvNetwork, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        self.dense1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.dense2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # (1) hidden conv layer\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        \n",
    "        # (2) hidden conv layer\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        \n",
    "        # (3) hidden linear layer\n",
    "        x = x.reshape(-1, 12*4*4)\n",
    "        x = self.dense1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # (4) hidden linear layer\n",
    "        x = self.dense2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # (5) output layer\n",
    "        x = self.out(x)\n",
    "        # x = F.softmax(x, dim=1) # This will be implemented within the loss function\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06de682",
   "metadata": {},
   "source": [
    "Notice that we use ```reshape()``` to give the right size of the flattened output of the convolutional layers to the linear layers. This is obtained as follows:\n",
    "\n",
    "Suppose\n",
    "- the input to convolutional layer is of size $n\\times n$,\n",
    "- each filter convolutional layer is of size $k\\times k$,\n",
    "- the convolution is padded with $p$, and has stride $s$.\n",
    "\n",
    "The output channel of the convolutional layer has size $m\\times m$, where:\n",
    "$$\n",
    "    m = \\frac{n-k+2p}{s} + 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e501451f",
   "metadata": {},
   "source": [
    "Let's use the ```forward()``` method to see what happens to an input to the network. We start with importing the FashionMNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7b3571fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='./../datasets',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97835434",
   "metadata": {},
   "source": [
    "We create an instance of the ```ConvNetwork``` class to use ```images``` as the input to the instance of the network. Before we instantiate, we will turn the ```autograd``` tags of the variable to ```False``` to stop the dynamic creation of the computational graph for backpropagation. We will see backpropagation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "994ca3b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fa4a87603d0>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "af946bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNetwork(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (dense1): Linear(in_features=192, out_features=120, bias=True)\n",
       "  (dense2): Linear(in_features=120, out_features=60, bias=True)\n",
       "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = ConvNetwork()\n",
    "network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239f465f",
   "metadata": {},
   "source": [
    "We pick one sample (image, label) pair, to see the output prediction of the network. The ```network``` is now callable that invokes the ```forward()``` method that we defined within the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "98d61ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(train_set))\n",
    "images, labels = sample\n",
    "\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0931bf1c",
   "metadata": {},
   "source": [
    "Notice the ```images``` object has a tensor with rank $3$, i.e., it has one image with one channel with that contains a $28\\times 28$ image. To load into the network, we need a rank $4$ tensor that tell the size of the batch. We use the ```unsqueeze()``` function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "338be2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0000)\n",
      "tensor([[0.0928, 0.1121, 0.0983, 0.0834, 0.1159, 0.0981, 0.1035, 0.0883, 0.0902, 0.1173]])\n"
     ]
    }
   ],
   "source": [
    "prediction = network(images.unsqueeze(0))\n",
    "print(F.softmax(prediction, dim=1).sum()) # Check distribution\n",
    "print(F.softmax(prediction, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1337f03d",
   "metadata": {},
   "source": [
    "Notice the predictions are all around $10\\%$, which is reasonable since the weights are randomly initialised and the prediction, given the input under randomly initialised parameters is uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "40792e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "tensor([9])\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(F.softmax(prediction, dim=1).argmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa81649",
   "metadata": {},
   "source": [
    "We can now do the same with a batch of images that can be created using the ```DataLoader```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "24cfe4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ffcca0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "images, labels = batch\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c8f4f893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([[0.0928, 0.1121, 0.0983, 0.0834, 0.1159, 0.0981, 0.1035, 0.0883, 0.0902, 0.1173],\n",
      "        [0.0931, 0.1121, 0.0989, 0.0829, 0.1160, 0.0983, 0.1047, 0.0886, 0.0890, 0.1165],\n",
      "        [0.0924, 0.1108, 0.0974, 0.0847, 0.1133, 0.0986, 0.1041, 0.0901, 0.0917, 0.1168],\n",
      "        [0.0926, 0.1111, 0.0975, 0.0845, 0.1142, 0.0987, 0.1042, 0.0898, 0.0905, 0.1169],\n",
      "        [0.0919, 0.1118, 0.0977, 0.0838, 0.1155, 0.0996, 0.1039, 0.0900, 0.0893, 0.1165],\n",
      "        [0.0924, 0.1112, 0.0989, 0.0831, 0.1166, 0.0980, 0.1047, 0.0891, 0.0887, 0.1172],\n",
      "        [0.0929, 0.1123, 0.0982, 0.0838, 0.1146, 0.0982, 0.1041, 0.0880, 0.0909, 0.1170],\n",
      "        [0.0930, 0.1116, 0.0990, 0.0834, 0.1171, 0.0973, 0.1042, 0.0889, 0.0882, 0.1172],\n",
      "        [0.0924, 0.1114, 0.0979, 0.0846, 0.1137, 0.0976, 0.1046, 0.0887, 0.0926, 0.1164],\n",
      "        [0.0929, 0.1125, 0.0992, 0.0837, 0.1139, 0.0974, 0.1055, 0.0875, 0.0912, 0.1162]])\n"
     ]
    }
   ],
   "source": [
    "predictions = network(images)\n",
    "print(F.softmax(predictions, dim=1).sum(dim=1)) # Check distribution\n",
    "print(F.softmax(predictions, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9daceced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(predictions, dim=1).argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "84604a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eabed8",
   "metadata": {},
   "source": [
    "Let's check how the predictions match with the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9798e47e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(predictions, dim=1).argmax(dim=1).eq(labels)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b5c7c648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(predictions, labels):\n",
    "    return F.softmax(predictions, dim=1).argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1fa50358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_correct(predictions, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
